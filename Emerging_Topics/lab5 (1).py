# -*- coding: utf-8 -*-
"""Lab5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JEJgwD5ZrDnrsexYmsEM8w-7JplvzpcG
"""

# Read data (dga_data.csv) from sample_data
import pandas as pd
dga_data = pd.read_csv('dga_data.csv')
dga_data.head()

# Encode the column isDGA to 0 or 1
dga_data['isDGA'] = dga_data['isDGA'].apply(lambda x: 1 if x == 'dga' else 0)
dga_data.head()

# Check number of isDGA values that are equal to 1
dga_data['isDGA'].value_counts()

"""### Exactly half (80,000) of the inputs are dga and the other half (80,000) are legit"""

# Count the number of each subclass
dga_data['subclass'].value_counts()

# Delete missing or duplicate values
dga_data = dga_data.dropna()
dga_data = dga_data.drop_duplicates()
dga_data.count()

"""### Eliminated two inputs whose hosts were missing"""

#Convert all domain and host strings to lowercase. Remove any invalid characters or additional dots
dga_data['domain'] = dga_data['domain'].apply(lambda x: x.lower())
dga_data['host'] = dga_data['host'].apply(lambda x: x.lower())
dga_data

# Remove dots and perform character based tokenization
dga_data['domain'] = dga_data['domain'].apply(lambda x: list(x.replace('.', '')))
dga_data['host'] = dga_data['host'].apply(lambda x: list(x.replace('.', '')))
dga_data

# Find average length of domains and longest length
avg_length = dga_data['domain'].apply(len).mean()
print(avg_length)
max_length = dga_data['domain'].apply(len).max()
print(max_length)

# Set maximum domain name length to 30 and pad shorter domains and truncate longer ones
max_length = 30
dga_data['domain'] = dga_data['domain'].apply(lambda x: x[:max_length] + ['0'] * (max_length - len(x)))
dga_data

# Create numerical encodings for the characters

from tensorflow.keras.preprocessing.text import Tokenizer

# Join the list of characters back into strings for tokenization.
hosts_str = dga_data['host'].apply(''.join)

# Create a character-level tokenizer.
tokenizer = Tokenizer(char_level=True)
tokenizer.fit_on_texts(hosts_str)

# Convert each host into its numeric sequence.
dga_data['host_encoded'] = tokenizer.texts_to_sequences(hosts_str)
dga_data

from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

# pad your host_encoded lists to shape (n_samples, MAX_LEN)
X = pad_sequences(
    dga_data['host_encoded'].tolist(),
    maxlen=max_length,
    padding='post',
    truncating='post',
    value=0
)

y = dga_data['isDGA'].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

#Create CNN Model

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout

# parameters (use the same MAX_LEN as your padding step)
MAX_LEN = 70
EMBEDDING_DIM = 50

# infer vocab size from your encoded data
VOCAB_SIZE = max(max(seq) for seq in dga_data['host_encoded']) + 1

cnn = Sequential([
    Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_LEN),
    Conv1D(filters=128, kernel_size=5, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

cnn.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

cnn.summary()

# Create an LSTM model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

# parameters (reuse the same MAX_LEN, EMBEDDING_DIM, VOCAB_SIZE)
lstm = Sequential([
    Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_LEN),
    LSTM(128, return_sequences=False),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

lstm.compile(optimizer='adam',
                   loss='binary_crossentropy',
                   metrics=['accuracy'])

lstm.summary()

import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Note: Ensure that the CNN model (variable `model`) from your previous cell is already defined.
history = cnn.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.1)

# --- Evaluate the Model on Test Data ---
# Get prediction probabilities on the test set.
y_pred_probs = cnn.predict(X_test)
# Convert probabilities to binary class predictions using a threshold of 0.5.
y_pred = (y_pred_probs > 0.5).astype(int)

# Compute evaluation metrics.
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Accuracy: {:.4f}".format(accuracy))
print("Precision: {:.4f}".format(precision))
print("Recall: {:.4f}".format(recall))
print("F1-score: {:.4f}".format(f1))

import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# --- Train the LSTM Model ---
history = lstm.fit(X_train, y_train, epochs=5, batch_size=256, validation_split=0.1)

# --- Evaluate the Model on Test Data ---
# Get prediction probabilities on the test set.
y_pred_probs = lstm.predict(X_test)
# Convert probabilities to binary class predictions using a threshold of 0.5.
y_pred = (y_pred_probs > 0.5).astype(int)

# Compute evaluation metrics.
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Accuracy: {:.4f}".format(accuracy))
print("Precision: {:.4f}".format(precision))
print("Recall: {:.4f}".format(recall))
print("F1-score: {:.4f}".format(f1))

"""## Now we will work on the subclass classification. We will use one hot encoding to map the categorical features of both legitimate and dga data separately.

### FIrst, we will create two models for CNN, one for legit subclasses and one for DGA subclasses. We will also do the same for LSTM.
"""

from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
)

# reuse these from your binary setup
MAX_LEN = 30
EMBEDDING_DIM = 50
VOCAB_SIZE = max(max(seq) for seq in dga_data['host_encoded']) + 1

# 1) DGA‐family subclass (e.g. gameoverdga, cryptolocker…)
dga_df = dga_data[dga_data['isDGA'] == 1]

# pad sequences
X_dga = pad_sequences(
    dga_df['host_encoded'].tolist(),
    maxlen=MAX_LEN,
    padding='post',
    truncating='post'
)

# encode labels to 0…(m−1) and then one-hot
le_dga = LabelEncoder().fit(dga_df['subclass'])
y_dga = le_dga.transform(dga_df['subclass'])
y_dga = to_categorical(y_dga)

# train/test split
X_train_dga, X_test_dga, y_train_dga, y_test_dga = train_test_split(
    X_dga, y_dga,
    test_size=0.2,
    stratify=y_dga,
    random_state=42
)

# build CNN
model_dga_sub = Sequential([
    Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LEN),
    Conv1D(128, 5, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(len(le_dga.classes_), activation='softmax')
])

model_dga_sub.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

print("=== DGA‐family subclass model ===")
model_dga_sub.summary()


# 2) Legitimate subclass (Alexa vs. nonAlexa)
legit_df = dga_data[dga_data['isDGA'] == 0]

X_legit = pad_sequences(
    legit_df['host_encoded'].tolist(),
    maxlen=MAX_LEN,
    padding='post',
    truncating='post'
)

le_legit = LabelEncoder().fit(legit_df['subclass'])
y_legit = le_legit.transform(legit_df['subclass'])
y_legit = to_categorical(y_legit)

X_train_legit, X_test_legit, y_train_legit, y_test_legit = train_test_split(
    X_legit, y_legit,
    test_size=0.2,
    stratify=y_legit,
    random_state=42
)

model_legit_sub = Sequential([
    Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LEN),
    Conv1D(64, 5, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(32, activation='relu'),
    Dropout(0.5),
    Dense(2, activation='softmax')
])

model_legit_sub.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

print("\n=== Legitimate subclass model ===")
model_legit_sub.summary()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

# reuse these from your subclass setup
MAX_LEN = 30
EMBEDDING_DIM = 50
VOCAB_SIZE = max(max(seq) for seq in dga_data['host_encoded']) + 1

# 1) DGA‐family subclass LSTM
model_dga_sub_lstm = Sequential([
    Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_LEN),
    LSTM(128),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(len(le_dga.classes_), activation='softmax')
])

model_dga_sub_lstm.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

print("=== DGA‐family subclass LSTM model ===")
model_dga_sub_lstm.summary()


# 2) Legitimate subclass LSTM (Alexa vs. nonAlexa)
model_legit_sub_lstm = Sequential([
    Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_LEN),
    LSTM(64),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.5),
    Dense(2, activation='softmax')
])

model_legit_sub_lstm.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

print("\n=== Legitimate subclass LSTM model ===")
model_legit_sub_lstm.summary()

"""### Now after creating the models, we will train and evaluate them"""

# Train, evaluate, and compare all 4 models

import matplotlib.pyplot as plt

# Hyperparameters
EPOCHS = 10
BATCH_SIZE = 128


# 3) DGA‑subclass CNN
history_dga_cnn = model_dga_sub.fit(
    X_train_dga, y_train_dga,
    validation_split=0.1,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE
)
loss_dga_cnn, acc_dga_cnn = model_dga_sub.evaluate(X_test_dga, y_test_dga)
print(f"DGA‑subclass CNN Test Accuracy: {acc_dga_cnn:.4f}")

# 4) DGA‑subclass LSTM
history_dga_lstm = model_dga_sub_lstm.fit(
    X_train_dga, y_train_dga,
    validation_split=0.1,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE
)
loss_dga_lstm, acc_dga_lstm = model_dga_sub_lstm.evaluate(X_test_dga, y_test_dga)
print(f"DGA‑subclass LSTM Test Accuracy: {acc_dga_lstm:.4f}")

# 5) Legit‑subclass CNN
history_legit_cnn = model_legit_sub.fit(
    X_train_legit, y_train_legit,
    validation_split=0.1,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE
)
loss_legit_cnn, acc_legit_cnn = model_legit_sub.evaluate(X_test_legit, y_test_legit)
print(f"Legit‑subclass CNN Test Accuracy: {acc_legit_cnn:.4f}")

# 6) Legit‑subclass LSTM
history_legit_lstm = model_legit_sub_lstm.fit(
    X_train_legit, y_train_legit,
    validation_split=0.1,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE
)
loss_legit_lstm, acc_legit_lstm = model_legit_sub_lstm.evaluate(X_test_legit, y_test_legit)
print(f"Legit‑subclass LSTM Test Accuracy: {acc_legit_lstm:.4f}")


# --- Plotting Comparison ---

# Subclass Classification Accuracy
tasks = ['DGA‑sub','Legit‑sub']
x = range(len(tasks))
width = 0.35

plt.figure(figsize=(8,4))
plt.bar([i-width/2 for i in x], [acc_dga_cnn, acc_legit_cnn], width, label='CNN')
plt.bar([i+width/2 for i in x], [acc_dga_lstm, acc_legit_lstm], width, label='LSTM')
plt.xticks(x, tasks)
plt.title('Subclass Classification Accuracy')
plt.ylabel('Accuracy')
plt.ylim(0,1)
plt.legend()
plt.show()